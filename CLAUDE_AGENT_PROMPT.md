# ðŸ¤– CLAUDE CODE WEB AGENT PROMPT

> **IMPORTANT**: Before starting, check if `.agent-completed` file exists in the root directory.
> If it exists, respond: "âœ… This task has already been completed. See README.md for details."
> **When finished**, create `.agent-completed` file with timestamp and summary.

---

# TASK: Build Numina Class Scraping & Ingestion System

## Repository Purpose
You are working on `numina-scrapers`, the TypeScript/Node.js service responsible for scraping group fitness class data from various providers (gyms, studios, class platforms) and pushing it to the `numina-backend` API.

## Current State
Repository initialized with basic README. Starting from scratch.

**Reference codebase**: The owner has a project called `godfather` (located at ~/WebstormProjects/godfather) which is a TypeScript/Puppeteer-based web scraper with cinema/Instagram automation. You can use similar architectural patterns but adapt for fitness class providers.

## Your Task
Create a modular, provider-based scraping system with the following features:

### Core Requirements

1. **Project Structure**
   - TypeScript with modern ES modules
   - Puppeteer for browser automation
   - Node.js 20+
   - Modular "provider adapter" architecture
   - SQLite database for tracking scrape operations
   - API client for pushing data to numina-backend

2. **Provider Adapter Pattern**
   ```typescript
   interface ProviderAdapter {
     name: string;
     scrapeClasses(options: ScrapeOptions): Promise<FitnessClass[]>;
     validateClass(classData: any): boolean;
   }
   ```

3. **Initial Provider Adapters** (implement 2-3)
   - **Mindbody** (major gym/studio booking platform)
   - **ClassPass** (if publicly scrapable)
   - **Local gym chain** (pick 1 local chain with public schedules - e.g., Equinox, 24 Hour Fitness)
   - Design for easy addition of new providers

4. **Data Model**
   ```typescript
   interface FitnessClass {
     name: string;
     description: string;
     datetime: Date;
     location: {
       name: string;
       address: string;
       lat: number;
       long: number;
     };
     trainer: string;
     intensity: number; // 1-10
     price: number;
     bookingUrl: string;
     providerId: string;
     providerName: string;
     capacity: number;
     tags: string[]; // yoga, hiit, spin, pilates, etc.
   }
   ```

5. **Scraping Infrastructure**
   - Chrome/Puppeteer management (headless mode)
   - Rate limiting and polite scraping (respect robots.txt)
   - Error handling and retry logic
   - Duplicate detection (don't re-scrape same classes)
   - Incremental scraping (only new/updated classes)

6. **Database for Operations Tracking**
   - SQLite database schema:
     - `scrape_runs`: track each scrape execution
     - `scraped_classes`: store raw scraped data before pushing to backend
     - `providers`: provider configuration and status
   - Track success/failure rates per provider

7. **Backend Integration**
   - API client to POST classes to `numina-backend` at `/api/v1/classes`
   - Batch uploads (e.g., 50 classes at a time)
   - Handle API errors gracefully
   - Idempotent uploads (avoid duplicates)

### Technical Constraints

- **Language**: TypeScript (Node.js 20+)
- **Automation**: Puppeteer for browser control
- **Database**: SQLite for local operations tracking
- **HTTP Client**: Fetch API or Axios for backend communication
- **Testing**: Jest with Puppeteer integration tests

### File Structure
```
numina-scrapers/
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.ts                    # Main CLI entry point
â”‚   â”œâ”€â”€ providers/                  # Provider adapters
â”‚   â”‚   â”œâ”€â”€ BaseProvider.ts
â”‚   â”‚   â”œâ”€â”€ MindbodyProvider.ts
â”‚   â”‚   â”œâ”€â”€ ClassPassProvider.ts
â”‚   â”‚   â””â”€â”€ EquinoxProvider.ts
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ ChromeManager.ts        # Puppeteer Chrome management
â”‚   â”‚   â”œâ”€â”€ Database.ts             # SQLite operations
â”‚   â”‚   â”œâ”€â”€ BackendClient.ts        # API client for numina-backend
â”‚   â”‚   â””â”€â”€ Scheduler.ts            # Cron-based scheduling
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ FitnessClass.ts
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ geocoding.ts            # Address â†’ lat/long
â”‚       â”œâ”€â”€ validation.ts           # Data validation
â”‚       â””â”€â”€ logger.ts               # Logging utility
â”œâ”€â”€ config/
â”‚   â””â”€â”€ providers.json              # Provider configuration
â”œâ”€â”€ tests/
â””â”€â”€ README.md
```

### Acceptance Criteria

1. âœ… Successfully scrapes classes from at least 2 providers
2. âœ… Extracts all required fields (name, time, location, price, booking URL)
3. âœ… Geocodes addresses to lat/long coordinates
4. âœ… Pushes scraped data to numina-backend API
5. âœ… Handles errors gracefully (network issues, page structure changes)
6. âœ… Tracks scraping operations in SQLite database
7. âœ… CLI interface: `npm run scrape -- --provider=mindbody`
8. âœ… Scheduler for automated daily scrapes
9. âœ… Deduplication logic (avoid duplicate classes)
10. âœ… Comprehensive logging and monitoring

### Deliverables

- Complete TypeScript scraping service
- At least 2 working provider adapters
- SQLite database with operations tracking
- Backend API integration
- CLI and scheduling system
- `.gitignore` file (exclude node_modules/, dist/, *.db, config/*.json with secrets)
- README with:
  - Setup instructions
  - How to add new providers
  - Configuration guide
  - Scraping best practices
- TODO.md with next providers to add

### How to Report Back

1. **Update README.md** with:
   - Quick start guide
   - Provider adapter documentation
   - How to add new providers (step-by-step)
   - CLI command reference
   - Scheduling setup instructions
   - Architecture diagram (ASCII or markdown)
   - List of 10+ potential future providers

2. **Create TODO.md** with prioritized next providers and features

3. **Create `.agent-completed` file** with content:
   ```
   Completed: [timestamp]
   Summary: Numina scrapers implemented successfully
   Providers: [list of implemented providers]
   Status: All acceptance criteria met
   Next: See TODO.md
   ```

4. **Commit and push** all changes with message:
   ```
   feat: Implement scraping infrastructure with initial providers

   ðŸ¤– Generated with [Claude Code](https://claude.com/claude-code)

   Co-Authored-By: Claude <noreply@anthropic.com>
   ```
